{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 16 GPT: Generative Pre-trained Transformer\n",
        "\n",
        "GPT is a model and approach developed by OpenAI. It is primarily known for its capabilities in generating coherent and contextually relevant text over long passages.\n",
        "\n",
        "* Architecture: GPT is based on the Transformer architecture. Unlike some other models that use both an encoder and a decoder, GPT exclusively utilizes the decoder part of the Transformer for its tasks.\n",
        "* Pre-training and Fine-tuning:\n",
        "    - Pre-training: GPT is first pre-trained on a large corpus of text (like books, articles, websites, etc.). During this phase, it learns to predict the next word in a sentence. This process enables the model to learn grammar, facts about the world, reasoning abilities, and even some level of common sense.\n",
        "    - Fine-tuning: After pre-training, the model can be fine-tuned on a specific task, such as translation, question-answering, or summarization, using a smaller, task-specific dataset.\n",
        "\n",
        "* Autoregressive Nature: GPT generates text in an autoregressive manner. This means it produces one word at a time and uses what it's generated so far as a context to generate the next word.\n",
        "\n",
        "#### Key Features:\n",
        "\n",
        "* Generative Abilities: As the name suggests, GPT excels at generating text. It can produce text that is often indistinguishable from what a human might write.\n",
        "* Few-Shot Learning: Introduced with GPT-3, this capability allows the model to perform tasks even when provided with very few examples (sometimes as few as one). By just specifying a task in natural language, GPT-3 can often understand and perform the task without explicit fine-tuning.\n",
        "* Versatility: Unlike many models that are trained for a specific task, GPT models, especially GPT-3, are versatile and can handle a wide range of tasks without task-specific training. This includes writing essays, answering questions, creating poetry, generating code, and much more.\n",
        "\n",
        "#### Versions:\n",
        "\n",
        "* GPT: The original model introduced by OpenAI.\n",
        "\n",
        "* GPT-2: A larger and more powerful version that garnered significant attention due to its impressive text generation capabilities. OpenAI initially withheld the fully-trained model due to concerns about misuse, but later released it given the broader community's responsible usage.\n",
        "\n",
        "* GPT-3: The third iteration with 175 billion parameters, making it one of the largest models ever created. It introduced the concept of few-shot and zero-shot learning, further advancing the state-of-the-art in various NLP tasks."
      ],
      "metadata": {
        "collapsed": false,
        "id": "MGfBFJ7hNZuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise: Exploring Creative Writing with GPT\n",
        "\n",
        "Your task is to use OpenAI's GPT model to generate creative content. You'll explore various prompts and settings to see how GPT responds and creates different outputs."
      ],
      "metadata": {
        "collapsed": false,
        "id": "82KYstUwNZuU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load GPT-2 Model and Tokenizer. GPT-2 is freely available in Hugging Face's model hub and is still highly effective."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ns-hikEHNZuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "#instalamos con pip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx_FaXs3djEx",
        "outputId": "9030b41d-a4b5-4416-cff1-45f89a7c6350"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.32.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "_072JkMyNZuX"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer # importamos GPT2 modelo preentrenado y un tokinizador que pasa de texto a token y viceversa\n",
        "\n",
        "model_name = 'gpt2-medium'  # You can start with 'gpt2' (smaller) and then experiment with larger models ;existen diferentes GPT2\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name) #usa from_pretained para cargar el tipo de largo del modelo\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)# igualmente hacemos para el token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Creative Content Function"
      ],
      "metadata": {
        "collapsed": false,
        "id": "J3ZXVRZONZuc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "#CODIFICACION DEL PROMPT\n",
        "def generate_creative_content(prompt, max_length=150, temperature=1.0): #\n",
        "    \"\"\"Generate creative content using GPT-2 based on a given prompt.\"\"\"\n",
        "\n",
        "    # Encode the prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    #Aquí se utiliza el tokenizador que cargamos anteriormente para convertir el texto del prompt en tokens (identificadores numéricos).\n",
        "#return_tensors='pt' indica que queremos que los tokens se devuelvan en un tensor de PyTorch.\n",
        "\n",
        "\n",
        "    # Generate text\n",
        "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "\n",
        "#Se usa el modelo GPT-2 (que también cargamos anteriormente) para generar texto basado en los input_ids del prompt.\n",
        "#max_length define cuántos tokens en total queremos que tenga la salida (incluyendo los tokens del prompt).\n",
        "#temperature afecta la aleatoriedad de la generación.\n",
        "#pad_token_id=tokenizer.eos_token_id define el token que se debe usar si el modelo genera un texto más corto que max_length. El eos_token_id es el token de \"fin de secuencia\" para GPT-2.\n",
        "\n",
        "    # Decode and print the generated text\n",
        "    generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "\n",
        "#Aquí se está decodificando el tensor de salida del modelo para convertirlo nuevamente en texto legible.\n",
        "#Se está utilizando un truco de indexación para extraer solo la parte del texto que el modelo generó (excluyendo el prompt original).\n",
        "#skip_special_tokens=True se asegura de que los tokens especiales (como el token de padding o el token de fin de secuencia) no se incluyan en el texto decodificado.\n",
        "\n",
        "    return generated_text\n",
        "    #esta función toma un prompt, lo codifica en tokens, utiliza el modelo GPT-2 para generar texto basado en ese prompt, decodifica el texto generado y luego lo devuelve."
      ],
      "metadata": {
        "id": "NuoTmElsNZud"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La función está diseñada para generar contenido creativo usando el modelo GPT-2 basado en un texto inicial (prompt) proporcionado.\n",
        "\n",
        "Parámetros:\n",
        "prompt: El texto inicial que se le da al modelo para que genere contenido a partir de él.\n",
        "max_length: La longitud máxima del texto generado. Por defecto es 150.\n",
        "temperature: Un valor que afecta la aleatoriedad de las decisiones tomadas por el modelo durante la generación de texto. Un valor más alto hace que el texto sea más aleatorio, mientras que un valor más bajo hace que sea más determinista. El valor predeterminado es 1."
      ],
      "metadata": {
        "id": "R4ABqOhQO-Qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Experiment:\n",
        "Use various prompts and observe GPT's creative capabilities.\n",
        "Change parameters like *max_length* and *temperature* to see their impact. (Note: A higher temperature value makes output more random, while a lower value makes it more deterministic.)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "ScOtCZW7NZue"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = [\n",
        "      \"focus on =fantasy, the promt is: Once upon a time, in a distant kingdom\",\n",
        "      \"focus on =Science Fiction , the promt is: In a dystopian future, where AI rules the world\",\n",
        "      \"focus on = History , the promt is:The last dinosaur on Earth was not like the others. It is\",\n",
        "      \"focus on = fantissa , the promt is: Deep in the waves of the ocean, a secret civilization\",\n",
        "]\n",
        "\n",
        "for prompt in prompts:\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    print(generate_creative_content(prompt))\n",
        "    print(\"\\n\" + \"-\"*50 + \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XobwzO_K96IK",
        "outputId": "4ebc31c0-61ae-4570-d233-c6545f6c966f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: focus on =fantasy, the promt is: Once upon a time, in a distant kingdom\n",
            ", there lived a king who was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: focus on =Science Fiction , the promt is: In a dystopian future, where AI rules the world\n",
            ", a group of scientists and engineers create a new type of machine, the \"AI-powered\" robot. The robot is designed to be able to think and reason, but it is also programmed to be able to kill. The robot is designed to be able to kill, but it is also programmed to be able to love. The robot is designed to be able to love, but it is also programmed to be able to hate. The robot is designed to be able to hate, but it is also programmed to be able to love. The robot is designed to be able to love, but it is also programmed to be able to hate.\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: focus on = History , the promt is:The last dinosaur on Earth was not like the others. It is\n",
            " a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the Jurassic Period. It was a dinosaur that lived in the\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Prompt: focus on = fantissa , the promt is: Deep in the waves of the ocean, a secret civilization\n",
            " is building a new world. The world is a place of wonder and wonder is the key to the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the secret. The secret is the\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discussion and Analysis:\n",
        "- Analyze the quality of the generated text: coherence, relevancy, and creativity.\n",
        "- Discuss how different prompts influence the direction of the story.\n",
        "- Experiment with custom prompts to generate different genres of creative content (e.g., horror, sci-fi, romance)."
      ],
      "metadata": {
        "collapsed": false,
        "id": "jyYGnhQVNZuf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tasks:\n",
        "* Use a larger GPT-2 variant (gpt2-large or gpt2-xl) and compare the quality of outputs.\n",
        "* Incorporate user feedback loops, where after getting an initial piece of text, they can provide a follow-up prompt to continue or steer the story."
      ],
      "metadata": {
        "collapsed": false,
        "id": "q1t_LreJNZug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos usar gpt2-xl"
      ],
      "metadata": {
        "id": "hY4Kxr-atZKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'gpt2-large'\n",
        "model_large = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer_large = GPT2Tokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "HcBeTHU3tWbg"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Incorporar bucles de retroalimentación del usuario:"
      ],
      "metadata": {
        "id": "1DpsOPIRuPEm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_creative_content(prompt, model, tokenizer, max_length=150, temperature=1.0):\n",
        "    \"\"\"Generate creative content using GPT-2 based on a given prompt.\"\"\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "    output = model.generate(input_ids, max_length=max_length, temperature=temperature, pad_token_id=tokenizer.eos_token_id)\n",
        "    generated_text = tokenizer.decode(output[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
        "    return generated_text\n"
      ],
      "metadata": {
        "id": "ljp0KJXK1V4F"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def interactive_story_sequential(model, tokenizer):\n",
        "    # Tomar el prompt inicial del usuario\n",
        "    story = input(\"Introduce el inicio de tu historia: \")\n",
        "\n",
        "    # Bucle para permitir al usuario continuar la historia\n",
        "    while True:\n",
        "        # Generar una respuesta basada en el prompt\n",
        "        generated_text = generate_creative_content(story, model=model, tokenizer=tokenizer, max_length=len(story.split()) + 50)  # Genera aproximadamente 50 palabras adicionales\n",
        "\n",
        "        # Mostrar la respuesta generada\n",
        "        print(f\"\\n{generated_text}\\n\")\n",
        "\n",
        "        # Preguntar al usuario si desea continuar o finalizar\n",
        "        choice = input(\"¿Quieres continuar la historia? (sí/no): \").lower()\n",
        "\n",
        "        if choice == 'no':\n",
        "            break\n",
        "\n",
        "        # Tomar el siguiente prompt del usuario y agregarlo a la historia existente\n",
        "        new_prompt = input(\"Introduce el siguiente fragmento o dirección de tu historia: \")\n",
        "        story += \" \" + new_prompt\n",
        "\n",
        "    print(\"¡Gracias por crear una historia!\")\n"
      ],
      "metadata": {
        "id": "ogtyvThZ1gzV"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interactive_story_sequential(model, tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1BOetGXs1i2z",
        "outputId": "7596a443-8060-4443-a458-ec5cd601cc16"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduce el inicio de tu historia: focus on =fantasy, the promt is: Once upon a time, in a distant kingdom\n",
            "\n",
            ", there lived a king who was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and he was a great warrior. He was a great warrior, and\n",
            "\n",
            "¿Quieres continuar la historia? (sí/no): si\n",
            "Introduce el siguiente fragmento o dirección de tu historia: and he died in the war to defend his kingdom\n",
            "\n",
            ". He was a great warrior, and he was a great warrior. He was a great warrior. He was a great warrior. He was a great warrior. He was a great warrior. He was a great warrior.\n",
            "\n",
            "¿Quieres continuar la historia? (sí/no): no\n",
            "¡Gracias por crear una historia!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}